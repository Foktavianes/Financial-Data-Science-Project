{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoSuchWindowException\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError\n",
    "import time\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import sched\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture the list of top 2000 companies with most H-1B Fillings\n",
    "#### Not running in headless mode as we need to manually close ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print('Timer started.')\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "# headless option to scrape to bypass advertisement that will popup halfway in the scraping process\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# navigate to the URL\n",
    "driver.get(\"https://h1bdata.info/\")\n",
    "\n",
    "# navigate to the companies page\n",
    "companies_link = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[2]/ul/li[3]\")\n",
    "companies_link.click()\n",
    "\n",
    "# locate the dropdown menu and click on the \"More...\" link\n",
    "dropdown_menu = driver.find_element(By.CLASS_NAME, \"dropdown-menu\")\n",
    "more_link = dropdown_menu.find_element(By.LINK_TEXT, \"More...\")\n",
    "more_link.click()\n",
    "\n",
    "# wait for the table element to be visible\n",
    "table_element = WebDriverWait(driver, 60).until(EC.visibility_of_element_located((By.XPATH, \"/html/body/div[2]/center/div/table\")))\n",
    "print('Company table loaded.')\n",
    "\n",
    "# convert the table to a pandas dataframe\n",
    "rows = table_element.find_elements(By.XPATH, \"./tbody/tr\")\n",
    "company_data = []\n",
    "\n",
    "# extract data from each row of the table\n",
    "for row_element in rows:\n",
    "    row = {}\n",
    "    columns = row_element.find_elements(By.XPATH, \"./td\")\n",
    "    print(len(columns))  # add this line to see how many columns there are in each row\n",
    "    if len(columns) < 4:\n",
    "        continue\n",
    "    row['Rank'] = columns[0].text\n",
    "    row['Company Name'] = columns[1].text\n",
    "    row['H-1B Filings'] = columns[2].text\n",
    "    row['Average Salary'] = columns[3].text\n",
    "    print(row)  # add this line to see what the row data looks like\n",
    "    company_data.append(row)\n",
    "\n",
    "company = pd.DataFrame(company_data)\n",
    "\n",
    "# save the dataframe as a CSV\n",
    "company.to_csv(\"companies.csv\", index=False)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the URL link for each individual companies\n",
    "#### Not running in headless mode either to manually close ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL =\"https://h1bdata.info/\"\n",
    "\n",
    "# Create an empty list to store the data\n",
    "table_for_links = []\n",
    "\n",
    "# Set the headless option\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Install and configure the ChromeDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "# Navigate to the URL\n",
    "driver.get(\"https://h1bdata.info/\")\n",
    "\n",
    "print('First link, https://h1bdata.info/, is working.')\n",
    "\n",
    "# navigate to the companies page\n",
    "companies_link = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[2]/ul/li[3]\")\n",
    "companies_link.click()\n",
    "\n",
    "# locate the dropdown menu and click on the \"More...\" link\n",
    "dropdown_menu = driver.find_element(By.CLASS_NAME, \"dropdown-menu\")\n",
    "more_link = dropdown_menu.find_element(By.LINK_TEXT, \"More...\")\n",
    "more_link.click()\n",
    "\n",
    "print('Second link, https://h1bdata.info/topcompanies.php, is working.')\n",
    "\n",
    "# List of potential XPath expressions for the overlay\n",
    "overlay_xpath_list = [\n",
    "    \"/html/body/div/div/div[1]\",\n",
    "    \"/html/body/div/div\",\n",
    "    # Add more XPath expressions if needed\n",
    "]\n",
    "\n",
    "# Flag to indicate if the overlay is successfully closed\n",
    "overlay_closed = False\n",
    "\n",
    "# Iterate through the list of XPath expressions and try to close the overlay\n",
    "for overlay_xpath in overlay_xpath_list:\n",
    "    try:\n",
    "        overlay = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, overlay_xpath)))\n",
    "        close_button = WebDriverWait(overlay, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"#dismiss-button > div > span\")))\n",
    "        close_button.click()\n",
    "        print(\"Overlay closed using XPath expression:\", overlay_xpath)\n",
    "        overlay_closed = True\n",
    "        break  # Exit the loop once the overlay is closed\n",
    "\n",
    "    except:\n",
    "        continue  # Try the next XPath expression if this one fails\n",
    "\n",
    "# If the overlay is not closed, handle the exception\n",
    "if not overlay_closed:\n",
    "    print(\"Overlay not found. Proceeding without closing.\")\n",
    "\n",
    "\n",
    "# wait for the table element to be visible\n",
    "table_element = WebDriverWait(driver, 240).until(EC.visibility_of_element_located((By.XPATH, \"/html/body/div[2]/center/div/table\")))\n",
    "print('Company table loaded.')\n",
    "\n",
    "\n",
    "for i in range(2, 2002):\n",
    "    try:\n",
    "        print(f\"loop {i}\")\n",
    "        #start scraping from the second company since the first companyï¼Œinfosys limited, does not load properly\n",
    "        # Scroll the page to bring the element into view\n",
    "        element = driver.find_element(By.XPATH, f\"/html/body/div[2]/center/div/table/tbody/tr[{i}]/td[2]/a\")\n",
    "\n",
    "        # Get the text referring to the URL link\n",
    "        link_text = element.text\n",
    "\n",
    "        # Get the href attribute value\n",
    "        url_link = element.get_attribute(\"href\")\n",
    "\n",
    "        # Print the URL link\n",
    "        print(\"Link Text:\",link_text,\"URL link:\", url_link)\n",
    "\n",
    "        # Add the data to the list\n",
    "        table_for_links.append({'Link Text': link_text, 'URL Link': url_link})\n",
    "    except NoSuchElementException as e:\n",
    "        print(f\"Element not found for company {i-1}. Skipping to next loop. NoSuchElement\")\n",
    "        continue\n",
    "\n",
    "# Convert the list into a DataFrame\n",
    "df = pd.DataFrame(table_for_links)\n",
    "df.to_csv(\"company_links.csv\")\n",
    "df.head()\n",
    "\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_for_links=pd.read_csv('company_links.csv',index_col=0)\n",
    "table_for_links.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Scraper Function\n",
    "#### Running in headless mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(year, link_text, url_link, lazy_load = 120):\n",
    "    url = f'https://h1bdata.info/index.php?year={year}&em={url_link}'\n",
    "\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "\n",
    "        # Install and configure the ChromeDriver\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "        # Navigate to the URL\n",
    "        driver.get(url)\n",
    "\n",
    "        table_element = None  # Initialize table_element to None\n",
    "\n",
    "        try:\n",
    "            # Scroll down to the bottom of the page in steps to trigger lazy loading (if any)\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            while True:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(180)  # Wait to load page\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "                last_height = new_height\n",
    "\n",
    "            # After scrolling to the bottom, go back to top of the page\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            # Wait for the table element to be visible\n",
    "            table_element = WebDriverWait(driver, lazy_load).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, \"//table[@id='myTable']\")))\n",
    "            print(f\"Table element is visible for {link_text} in year {year}\")\n",
    "        except TimeoutException:\n",
    "            print(f\"Table element is not visible for {link_text} in year {year} within the timeout.\")\n",
    "            driver.quit()\n",
    "            return False\n",
    "\n",
    "        if table_element is not None:\n",
    "            # Retrieve the rows of the table\n",
    "            rows = table_element.find_elements(By.XPATH, \"./tbody/tr\")\n",
    "\n",
    "            company_data = []\n",
    "            for row_element in rows:\n",
    "                columns = row_element.find_elements(By.XPATH, \"./td\")\n",
    "                if columns is None or len(columns) < 6:\n",
    "                    continue\n",
    "                row_data = {\n",
    "                    'Employer': columns[0].text,\n",
    "                    'Job Title': columns[1].text if len(columns) > 1 else '',\n",
    "                    'Base Salary': columns[2].text if len(columns) > 2 else '',\n",
    "                    'Location': columns[3].text if len(columns) > 3 else '',\n",
    "                    'Submit Date': columns[4].text if len(columns) > 4 else '',\n",
    "                    'Start Date': columns[5].text if len(columns) > 5 else ''\n",
    "                }\n",
    "                company_data.append(row_data)\n",
    "\n",
    "            # Close the ChromeDriver\n",
    "            driver.quit()\n",
    "\n",
    "            # Create a DataFrame from the collected data\n",
    "            df_company = pd.DataFrame(company_data)\n",
    "            \n",
    "            # Sanitize the link_text\n",
    "            sanitized_link_text = ScrapeScheduler.sanitize_filename(link_text)\n",
    "\n",
    "            # Save the DataFrame as a CSV file with the company name and year as the file name\n",
    "            filename = f\"{sanitized_link_text}_{year}.csv\"\n",
    "            df_company.to_csv(filename, index=False)\n",
    "            print(f\"Saved DataFrame for {link_text} in year {year} as {filename}\")\n",
    "\n",
    "            return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for {link_text} in year {year}: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Scheduler class to scrape the h1b visa website at scheduled times in concurrent processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeScheduler:\n",
    "    def __init__(self, table_for_links, year_range):\n",
    "        self.table_for_links = table_for_links\n",
    "        self.year_range = year_range\n",
    "        self.missing_links_years = None\n",
    "        self.scheduler = sched.scheduler(time.time, time.sleep)\n",
    "        \n",
    "    @staticmethod\n",
    "    def sanitize_filename(filename):\n",
    "        return re.sub(r'[\\\\/:\"*?<>|]', '', filename) #sanitize special characters for csv filenames\n",
    "\n",
    "    def get_missing_years_and_links(self, directory):\n",
    "        link_texts = self.table_for_links['Link Text'].unique()\n",
    "        all_years = set(self.year_range)\n",
    "\n",
    "        missing_links_years = []\n",
    "        for link_text in link_texts:\n",
    "            files = [f for f in os.listdir(directory) if f.startswith(link_text)]\n",
    "            found_years = {int(f.split('_')[1].split('.')[0]) for f in files}\n",
    "            missing_years = all_years - found_years\n",
    "            for year in missing_years:\n",
    "                missing_links_years.append({'Link Text': link_text, 'URL Link': self.table_for_links.loc[self.table_for_links['Link Text'] == link_text, 'URL Link'].iloc[0], 'Year': year})\n",
    "\n",
    "        return pd.DataFrame(missing_links_years)\n",
    "\n",
    "    def run_scrape_data_with_years(self, mode='normal', lazy_load=120):\n",
    "        if mode == 'missing':\n",
    "            self.missing_links_years = self.get_missing_years_and_links(os.getcwd())\n",
    "        elif mode == 'normal':\n",
    "            self.missing_links_years = self.table_for_links.copy()\n",
    "            self.missing_links_years['Year'] = [self.year_range] * len(self.missing_links_years)  # Assign the year range list to each row\n",
    "            self.missing_links_years = self.missing_links_years.explode('Year')  # Create a new row for each year\n",
    "\n",
    "        if self.missing_links_years.empty:\n",
    "            print(\"No missing years and links to scrape.\")\n",
    "            return\n",
    "\n",
    "        max_workers = 40\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for _, row in self.missing_links_years.iterrows():\n",
    "                link_text = row['Link Text']\n",
    "                url_link = row['URL Link'][44:]\n",
    "                year = row['Year']\n",
    "                future = executor.submit(scrape_data, year, link_text, url_link, lazy_load)\n",
    "                futures.append((future, link_text, year))\n",
    "\n",
    "            for future, link_text, year in futures:\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        # If scraping was successful, remove this row from the DataFrame\n",
    "                        self.missing_links_years = self.missing_links_years[self.missing_links_years['Link Text'] != link_text]\n",
    "                    else:\n",
    "                        print(f\"Failed to scrape data for {link_text} in year {year}.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while processing {link_text} in year {year}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate scheduler \n",
    "scheduler = ScrapeScheduler(table_for_links, range(2023, 2011, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data to capture h1b data per company by year\n",
    "scheduler.run_scrape_data_with_years(mode='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run this to capture missing company & year combos + combat lazy loading issues\n",
    "scheduler.run_scrape_data_with_years(mode='missing', lazy_load = 720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge scraped csv files into combined dataframe and save as pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_files():\n",
    "    dfs = []  # an empty list to store the data frames\n",
    "    for filename in os.listdir('.'):  # '.' means current directory\n",
    "        if filename.endswith('.csv'):\n",
    "            try:\n",
    "                dfs.append(pd.read_csv(filename))\n",
    "            except EmptyDataError:\n",
    "                print(f'Skipped empty file: {filename}')\n",
    "                continue\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df.to_pickle(\"combined_data.pkl\")  # Save the combined dataframe to a pickle file\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combine_csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_filtered_dataframe(df: pd.DataFrame, keyword1: str, keyword2: str) -> pd.DataFrame:\n",
    "    # create regex patterns\n",
    "    keyword1_pattern = re.compile(keyword1, re.IGNORECASE)\n",
    "    keyword2_pattern = re.compile(keyword2, re.IGNORECASE)\n",
    "\n",
    "    # lower-case dataframe columns for comparison\n",
    "    df_lower = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "    # filter rows based on whether they contain either keyword\n",
    "    matches_keyword1 = df_lower['Employer'].apply(lambda x: bool(keyword1_pattern.search(x)) if isinstance(x, str) else False) | df_lower['Job Title'].apply(lambda x: bool(keyword1_pattern.search(x)) if isinstance(x, str) else False)\n",
    "    matches_keyword2 = df_lower['Employer'].apply(lambda x: bool(keyword2_pattern.search(x)) if isinstance(x, str) else False) | df_lower['Job Title'].apply(lambda x: bool(keyword2_pattern.search(x)) if isinstance(x, str) else False)\n",
    "\n",
    "    # return slice of original df where either conditions is True\n",
    "    return df[matches_keyword1 & matches_keyword2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_filtered_dataframe(combined_df, keyword1 = \"jp\" ,keyword2 = \"quant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
